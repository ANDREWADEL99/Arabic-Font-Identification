{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage.io as io\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import skimage as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import data\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = load_images_from_folder('ACdata_base')\n",
    "names = np.loadtxt(\"ACdata_base/names.txt\", dtype=str)\n",
    "labels = np.zeros(len(images))\n",
    "###################### LAbels ############################\n",
    "labels[:190] = 1\n",
    "labels[190:380] = 2\n",
    "labels[380:560] = 3\n",
    "labels[560:745] = 4\n",
    "labels[745:940] = 5\n",
    "labels[940:1120] = 6\n",
    "labels[1120:1305] = 7\n",
    "labels[1305:1495] = 8\n",
    "labels[1495:1684] = 9\n",
    "###########################################################\n",
    "print(labels[190])\n",
    "#for name in names:\n",
    "#    labels.append(name[4:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images,titles=None):\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show()\n",
    "\n",
    "def PreProcessing(images):\n",
    "    threshed = []\n",
    "    for img in images:\n",
    "        # (1) RGB to Gray\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # (2) threshold\n",
    "        threshed.append(cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1])\n",
    "        #pts = cv2.findNonZero(threshed)\n",
    "        #ret = cv2.minAreaRect(pts)\n",
    "    \n",
    "    #The data has a correct orientation already.\n",
    "    '''\n",
    "    # (3) minAreaRect on the nonzeros\n",
    "    H, W = img.shape[:2]\n",
    "    (cx, cy), (w, h), ang = ret\n",
    "\n",
    "    if (H > W and w > h) or (H < W and w < h):\n",
    "        w, h = h, w\n",
    "        ang += 90\n",
    "\n",
    "    # (4) Find rotated matrix, do rotation\n",
    "    M = cv2.getRotationMatrix2D((cx, cy), ang, 1.0)\n",
    "    rotated = cv2.warpAffine(threshed, M, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # (5) find and draw the upper and lower boundary of each lines\n",
    "    hist = cv2.reduce(rotated, 1, cv2.REDUCE_AVG).reshape(-1)\n",
    "    th = 2\n",
    "    upper = [y for y in range(H-1) if hist[y] <= th and hist[y+1] > th]\n",
    "    lower = [y for y in range(H-1) if hist[y] > th and hist[y+1] <= th]\n",
    "    \n",
    "    line = []\n",
    "    line.append(rotated[upper[0]-5:lower[0]+5, :])\n",
    "    '''\n",
    "    return threshed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show_images([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(images)\n",
    "lines = PreProcessing(images)\n",
    "#show_images([line[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lpq(img,winSize=3,freqestim=1,mode='nh'):\n",
    "    rho=0.90\n",
    "\n",
    "    STFTalpha=1/winSize  # alpha in STFT approaches (for Gaussian derivative alpha=1)\n",
    "    sigmaS=(winSize-1)/4 # Sigma for STFT Gaussian window (applied if freqestim==2)\n",
    "    sigmaA=8/(winSize-1) # Sigma for Gaussian derivative quadrature filters (applied if freqestim==3)\n",
    "\n",
    "    convmode='valid' # Compute descriptor responses only on part that have full neigborhood. Use 'same' if all pixels are included (extrapolates np.image with zeros).\n",
    "\n",
    "    img=np.float64(img) # Convert np.image to double\n",
    "    r=(winSize-1)/2 # Get radius from window size\n",
    "    x=np.arange(-r,r+1)[np.newaxis] # Form spatial coordinates in window\n",
    "\n",
    "    if freqestim==1:  #  STFT uniform window\n",
    "        #  Basic STFT filters\n",
    "        w0=np.ones_like(x)\n",
    "        w1=np.exp(-2*np.pi*x*STFTalpha*1j)\n",
    "        w2=np.conj(w1)\n",
    "\n",
    "    ## Run filters to compute the frequency response in the four points. Store np.real and np.imaginary parts separately\n",
    "    # Run first filter\n",
    "    filterResp1=convolve2d(convolve2d(img,w0.T,convmode),w1,convmode)\n",
    "    filterResp2=convolve2d(convolve2d(img,w1.T,convmode),w0,convmode)\n",
    "    filterResp3=convolve2d(convolve2d(img,w1.T,convmode),w1,convmode)\n",
    "    filterResp4=convolve2d(convolve2d(img,w1.T,convmode),w2,convmode)\n",
    "\n",
    "    # Initilize frequency domain matrix for four frequency coordinates (np.real and np.imaginary parts for each frequency).\n",
    "    freqResp=np.dstack([filterResp1.real, filterResp1.imag,\n",
    "                        filterResp2.real, filterResp2.imag,\n",
    "                        filterResp3.real, filterResp3.imag,\n",
    "                        filterResp4.real, filterResp4.imag])\n",
    "\n",
    "    ## Perform quantization and compute LPQ codewords\n",
    "    inds = np.arange(freqResp.shape[2])[np.newaxis,np.newaxis,:]\n",
    "    LPQdesc=((freqResp>0)*(2**inds)).sum(2)\n",
    "\n",
    "    ## Switch format to uint8 if LPQ code np.image is required as output\n",
    "    if mode=='im':\n",
    "        LPQdesc=np.uint8(LPQdesc)\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(\"Original image\")\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(LPQdesc, cmap='gray')\n",
    "        plt.title(\"lpq\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    ## Histogram if needed\n",
    "    if mode=='nh' or mode=='h':\n",
    "        LPQdesc=np.histogram(LPQdesc.flatten(),range(256))[0]\n",
    "\n",
    "    ## Normalize histogram if needed\n",
    "    if mode=='nh':\n",
    "        LPQdesc=LPQdesc/LPQdesc.sum()\n",
    "\n",
    "    return LPQdesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_FE = []\n",
    "X_test_FE = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(lines, labels, test_size=0.2, random_state=42)\n",
    "for i in X_train:\n",
    "    X_train_FE.append(lpq(i))\n",
    "\n",
    "for i in X_test:\n",
    "    X_test_FE.append(lpq(i))\n",
    "    \n",
    "#clf = RandomForestClassifier(max_depth=10, random_state=42).fit(X_train_FE, y_train)\n",
    "#clf.predict_proba(X_test_FE[:1])\n",
    "\n",
    "# clf.predict(X_test_FE)\n",
    "# clf.score(X_train_FE, y_train)\n",
    "##########################################\n",
    "#clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "#clf.fit(X, y)\n",
    "# define lists to collect scores\n",
    "train_scores, test_scores = list(), list()\n",
    "train_scores2, test_scores2 = list(), list()\n",
    "train_scores3, test_scores3 = list(), list()\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(1, 15)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    "    clf = RandomForestClassifier(max_depth=i, random_state=42).fit(X_train_FE, y_train)\n",
    "    # evaluate on the train dataset\n",
    "    clf.predict(X_train_FE)\n",
    "    train_acc = clf.score(X_train_FE, y_train)\n",
    "    train_scores.append(train_acc)\n",
    "    # evaluate on the test dataset\n",
    "    clf.predict(X_test_FE)\n",
    "    test_acc = clf.score(X_test_FE, y_test)\n",
    "    test_scores.append(test_acc)\n",
    "    # summarize progress\n",
    "    print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    "    clf2 = MLPClassifier(random_state=42, max_iter=i*200).fit(X_train_FE, y_train)\n",
    "    # evaluate on the train dataset\n",
    "    clf2.predict(X_train_FE)\n",
    "    train_acc2 = clf2.score(X_train_FE, y_train)\n",
    "    train_scores2.append(train_acc2)\n",
    "    # evaluate on the test dataset\n",
    "    clf2.predict(X_test_FE)\n",
    "    test_acc2 = clf2.score(X_test_FE, y_test)\n",
    "    test_scores2.append(test_acc2)\n",
    "    # summarize progress\n",
    "    print('>%d, train: %.3f, test: %.3f' % (i, train_acc2, test_acc2))\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    "    clf3 = make_pipeline(StandardScaler(),LinearSVC(random_state=42, tol=1e-4,max_iter=i*200)).fit(X_train_FE,y_train)\n",
    "    # evaluate on the train dataset\n",
    "    clf3.predict(X_train_FE)\n",
    "    train_acc3 = clf2.score(X_train_FE, y_train)\n",
    "    train_scores3.append(train_acc3)\n",
    "    # evaluate on the test dataset\n",
    "    clf3.predict(X_test_FE)\n",
    "    test_acc3 = clf3.score(X_test_FE, y_test)\n",
    "    test_scores3.append(test_acc3)\n",
    "    # summarize progress\n",
    "    print('>%d, train: %.3f, test: %.3f' % (i, train_acc3, test_acc3))\n",
    "# plot of train and test scores vs tree depth\n",
    "plt.plot(values, train_scores, '-o', label='Train RF')\n",
    "plt.plot(values, train_scores2, '-o', label='Train MLP')\n",
    "plt.plot(values, train_scores3, '-o', label='Train SVM')\n",
    "plt.plot(values, test_scores, '-o', label='Test RF')\n",
    "plt.plot(values, test_scores2, '-o', label='Test MLP')\n",
    "plt.plot(values, test_scores3, '-o', label='Test SVM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read imgs and preprocess\n",
    "new_test_images = load_images_from_folder('New_DB')\n",
    "new_test_images_proc = PreProcessing(new_test_images)\n",
    "new_test_features = []\n",
    "for i in new_test_images_proc:\n",
    "    new_test_features.append(lpq(i))\n",
    "# labels\n",
    "new_test_labels = np.zeros(len(new_test_images))\n",
    "new_test_labels[:13] = 1 \n",
    "new_test_labels[13:24] = 2\n",
    "new_test_labels[24:35] = 3\n",
    "new_test_labels[35:46] = 4\n",
    "new_test_labels[46:56] = 5\n",
    "new_test_labels[56:66] = 6\n",
    "new_test_labels[66:75] = 7\n",
    "new_test_labels[75:84] = 8\n",
    "new_test_labels[84:96] = 9\n",
    "\n",
    "\n",
    "# clf = RandomForestClassifier(max_depth=4, random_state=1).fit(X_train_FE, y_train)\n",
    "# clf2 = MLPClassifier(random_state=42, max_iter=300).fit(X_train_FE, y_train)\n",
    "# evaluate on the train dataset\n",
    "#clf.predict(new_test_features)\n",
    "# score1 = clf.score(new_test_features, new_test_labels)\n",
    "# score2 = clf2.score(new_test_features, new_test_labels)\n",
    "# print(\"Score1 =\",score1)\n",
    "# print(\"Score2 =\",score2)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-8)).fit(X_train_FE,y_train)\n",
    "#Pipeline(steps=[('standardscaler', StandardScaler()),('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
    "\n",
    "clf.predict(X_test_FE)\n",
    "clf.score(X_test_FE,y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
